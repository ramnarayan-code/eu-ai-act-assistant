import os
import logging
import traceback
import asyncio

from typing import Optional

# LlamaIndex imports
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.core.agent.workflow import FunctionAgent
from llama_index.core.tools import BaseTool

from ragas.metrics import AnswerRelevancy, Faithfulness
from ragas import SingleTurnSample, EvaluationDataset
from ragas import evaluate
from ragas.llms import LlamaIndexLLMWrapper

from rag_knowledge_source import RAGKnowledgeSource

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

class EUIActRAGAgent:
    """
    RAG system specifically designed for the EU AI Act document with context-aware chunking
    """

    SYSTEM_PROMPT = """ Based on the EU AI Act document, 
        please answer the following question based on the context with specific references to 
        relevant articles, chapters, or sections when possible:
        
        Question: {question}
        context: {context}

        Please do not make up any answers. Please do not provide generic answers.
        Please do not use Internet knowledge.
        Please provide a comprehensive answer citing specific parts of the regulation.
    """

    def __init__(
        self,
        openai_api_key: str,
        embedding_model: str = "text-embedding-3-small",
        llm_model: str = "gpt-4o-mini",
        persist_dir: Optional[str] = None,
    ):
        """
        Initialize the RAG system

        Args:
            openai_api_key: OpenAI API key
            embedding_model: OpenAI embedding model to use
            llm_model: OpenAI LLM model to use
            persist_dir: Directory to persist the index
        """

        # Initialize embedding model
        self.embed_model = OpenAIEmbedding(
            model=embedding_model, api_key=openai_api_key
        )
        # Initialize LLM
        self.llm = OpenAI(model=llm_model, api_key=openai_api_key, temperature=0.1)
        self.evaluator = EUIActRAGSystemEvaluator()
        self.knowledge_source = RAGKnowledgeSource(openai_api_key=openai_api_key,
                                                 persist_dir=persist_dir)
       
    async def query(self, query: str, tools: list[BaseTool] = None) -> str:
        """
            Query the EU AI Act document with context-aware responses
        Args:
            query: The user query
            tools: Optional list of tools for the agent to use
        Returns:    
            The response from the RAG system    
        """
        context = [self.knowledge_source.get_context(query)]

        agent = FunctionAgent(
            tools=tools,
            llm=self.llm,
            system_prompt=self.SYSTEM_PROMPT.format(
                context=context, question=query
            ),
        )

        response = await agent.run(query)
        self.evaluator.evaluate(query, response, context)

        return response


class EUIActRAGSystemEvaluator:
    """
    Evaluation module for the EU AI Act RAG system
    """

    def __init__(self):
        self.llm = LlamaIndexLLMWrapper(OpenAI("gpt-4.1"))
        
    def evaluate(self, query: str, response: str, context_array: list[str]) -> dict:
        """
        Evaluate the response generated by the RAG system

        Args:
            query: The original query
            context_text: The context used for generating the response
            response: The generated response

        Returns:
            A dictionary with evaluation metrics
        """
        evaluation_dataset = EvaluationDataset(
            samples=[
                SingleTurnSample(
                    user_input=query,
                    retrieved_contexts=list(context_array),
                    response=str(response),
                )
            ]
        )

        result = evaluate(
            dataset=evaluation_dataset,
            metrics=[AnswerRelevancy(), Faithfulness()],
            llm=self.llm,
        )

        return result


async def main():
    """
    Example usage of the EU AI Act RAG system
    """

    # Configuration
    PERSIST_DIR = "./eu_ai_act_index"  # Directory to save/load the index

    # Initialize the RAG system
    rag_agent = EUIActRAGAgent(
        openai_api_key=OPENAI_API_KEY, persist_dir=PERSIST_DIR
    )

    try:
        # https://github.com/run-llama/llama_index/issues/12603
        # Example queries
        example_queries = [
            # "My AI application collects PII data, is my application high risk? say yes or no. briefly advise",
            # "What is AI Literacy?",
            # "What is Regulation (EU) 2024/1689?",
            # "Summarize the whole document in 5 bullet points",
            "List me top 3 prohibited AI practices?",
            # "List the classification of AI systems as high risk in bullet points",
            # "Difference between EU AI Act and US AI Act"
        ]

        logger.info("\n" + "=" * 60)
        logger.info("EXAMPLE QUERIES AND RESPONSES")
        logger.info("=" * 60)

        for query in example_queries:
            logger.info(f"\nQ: {query}")
            logger.info("-" * 40)
            response = await rag_agent.query(query)
            logger.info(f"A: {response}")
            logger.info("\n" + "=" * 60)
    except Exception as e:
        traceback.print_exc()
        logger.error(e)
        logger.error("Please check your OpenAI API key and file paths.")


if __name__ == "__main__":
    asyncio.run(main())