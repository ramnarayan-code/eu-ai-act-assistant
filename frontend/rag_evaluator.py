from ragas.metrics import AnswerRelevancy, Faithfulness
from ragas import SingleTurnSample, EvaluationDataset
from ragas import evaluate
from ragas.llms import LlamaIndexLLMWrapper
from llama_index.llms.openai import OpenAI

class RAGEvaluator:
    """
    Evaluation module for the EU AI Act RAG system
    """

    def __init__(self):
        self.llm = LlamaIndexLLMWrapper(OpenAI("gpt-4.1"))
        
    def evaluate(self, query: str, response: str, context_array: list[str]) -> dict:
        """
        Evaluate the response generated by the RAG system

        Args:
            query: The original query
            context_text: The context used for generating the response
            response: The generated response

        Returns:
            A dictionary with evaluation metrics
        """
        evaluation_dataset = EvaluationDataset(
            samples=[
                SingleTurnSample(
                    user_input=query,
                    retrieved_contexts=list(context_array),
                    response=str(response),
                )
            ]
        )

        result = evaluate(
            dataset=evaluation_dataset,
            metrics=[AnswerRelevancy(), Faithfulness()],
            llm=self.llm,
        )

        return result